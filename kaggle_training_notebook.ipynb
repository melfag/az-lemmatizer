{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azerbaijani Lemmatizer - Cloud Training\n",
    "\n",
    "**Before running:**\n",
    "1. Enable GPU: Settings ‚Üí Accelerator ‚Üí GPU T4 x2\n",
    "2. Upload code as Kaggle Dataset\n",
    "3. Upload data files as separate Dataset\n",
    "4. Add both datasets to this notebook (right sidebar)\n",
    "\n",
    "**Expected time:** 8-10 hours for 20 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers torch pyyaml tqdm sentencepiece\n",
    "\n",
    "# Verify GPU\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU! Enable it in Settings ‚Üí Accelerator ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Code and Data\n",
    "\n",
    "**Option A:** If you uploaded code as Kaggle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy code from input dataset\n",
    "!cp -r /kaggle/input/azerbaijani-lemmatizer/* /kaggle/working/\n",
    "%cd /kaggle/working\n",
    "\n",
    "# Verify structure\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option B:** If you're cloning from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/yourusername/azerbaijani-lemmatizer.git\n",
    "%cd azerbaijani-lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Update Config for Kaggle Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load config\n",
    "config_path = 'configs/improved_training.yaml'\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update paths to use Kaggle input dataset\n",
    "# IMPORTANT: Replace 'your-dataset-name' with your actual dataset name!\n",
    "DATA_PATH = '/kaggle/input/az-lemmatizer-data'  # Change this!\n",
    "\n",
    "config['data']['train_path'] = f'{DATA_PATH}/moraz_500k_train_filtered.json'\n",
    "config['data']['val_path'] = f'{DATA_PATH}/moraz_500k_val.json'\n",
    "config['data']['test_path'] = f'{DATA_PATH}/ud_test.json'\n",
    "config['vocab_path'] = f'{DATA_PATH}/char_vocab.json'\n",
    "\n",
    "# Save updated config\n",
    "with open('configs/kaggle_training.yaml', 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"‚úì Config updated for Kaggle paths\")\n",
    "print(f\"\\nData directory: {DATA_PATH}\")\n",
    "print(\"\\nVerifying data files...\")\n",
    "\n",
    "import os\n",
    "for key, path in config['data'].items():\n",
    "    if isinstance(path, str) and path.endswith('.json'):\n",
    "        exists = os.path.exists(path)\n",
    "        print(f\"  {key}: {'‚úì' if exists else '‚úó'} {path}\")\n",
    "\n",
    "vocab_exists = os.path.exists(config['vocab_path'])\n",
    "print(f\"  vocab: {'‚úì' if vocab_exists else '‚úó'} {config['vocab_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "!python scripts/train.py --config configs/kaggle_training.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Results\n",
    "\n",
    "**IMPORTANT:** Kaggle sessions expire! Save results to /kaggle/working/ so you can download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('/kaggle/working/results', exist_ok=True)\n",
    "\n",
    "# Copy checkpoints\n",
    "checkpoint_dir = 'checkpoints/improved_training'\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    # Copy best model\n",
    "    if os.path.exists(f'{checkpoint_dir}/best_model.pt'):\n",
    "        shutil.copy(\n",
    "            f'{checkpoint_dir}/best_model.pt',\n",
    "            '/kaggle/working/results/best_model.pt'\n",
    "        )\n",
    "        print(\"‚úì Saved best model\")\n",
    "    \n",
    "    # Copy latest checkpoint\n",
    "    if os.path.exists(f'{checkpoint_dir}/latest.pt'):\n",
    "        shutil.copy(\n",
    "            f'{checkpoint_dir}/latest.pt',\n",
    "            '/kaggle/working/results/latest.pt'\n",
    "        )\n",
    "        print(\"‚úì Saved latest checkpoint\")\n",
    "    \n",
    "    # Copy training history\n",
    "    if os.path.exists(f'{checkpoint_dir}/training_history.json'):\n",
    "        shutil.copy(\n",
    "            f'{checkpoint_dir}/training_history.json',\n",
    "            '/kaggle/working/results/training_history.json'\n",
    "        )\n",
    "        print(\"‚úì Saved training history\")\n",
    "\n",
    "print(\"\\nüìÅ Results saved to /kaggle/working/results/\")\n",
    "print(\"   Download from: Data ‚Üí Output Files (right sidebar)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Evaluation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick evaluation on UD test set\n",
    "!python scripts/evaluate.py \\\n",
    "    --checkpoint /kaggle/working/results/best_model.pt \\\n",
    "    --test-data /kaggle/input/az-lemmatizer-data/ud_test.json \\\n",
    "    --config configs/kaggle_training.yaml \\\n",
    "    --output-dir /kaggle/working/results/evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Results\n",
    "\n",
    "After training completes:\n",
    "1. Go to: Data ‚Üí Output (right sidebar)\n",
    "2. Download `results/` folder\n",
    "3. Contains:\n",
    "   - `best_model.pt` - Best model checkpoint\n",
    "   - `latest.pt` - Latest checkpoint (for resuming)\n",
    "   - `training_history.json` - Loss curves\n",
    "   - `evaluation/` - Evaluation metrics (if you ran evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "**Session timeout:**\n",
    "- Kaggle sessions run for max 12 hours\n",
    "- Training should complete in ~8-10 hours\n",
    "- If it times out, resume from latest checkpoint:\n",
    "  ```python\n",
    "  !python scripts/train.py \\\n",
    "      --config configs/kaggle_training.yaml \\\n",
    "      --resume /kaggle/working/results/latest.pt\n",
    "  ```\n",
    "\n",
    "**Data not found:**\n",
    "- Check dataset is added to notebook (right sidebar)\n",
    "- Verify path in cell 3 matches your dataset name\n",
    "- List files: `!ls /kaggle/input/`\n",
    "\n",
    "**No GPU:**\n",
    "- Settings ‚Üí Accelerator ‚Üí GPU T4 x2\n",
    "- Save settings\n",
    "- Restart kernel\n",
    "\n",
    "**Out of memory:**\n",
    "- Reduce batch size in config:\n",
    "  ```python\n",
    "  config['training']['batch_size'] = 16  # Default is 32\n",
    "  ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
